{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline model\n",
    "\n",
    "Bertrand Thia (bt2513)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the librairies\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from keras.callbacks.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_labels = pd.read_csv('./train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "normal       350\n",
       "bacterial    350\n",
       "viral        350\n",
       "covid         77\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "normal       0.310559\n",
       "bacterial    0.310559\n",
       "viral        0.310559\n",
       "covid        0.068323\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels['label'].value_counts(normalize= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw in the previous notebook, our dataset is imbalanced and the number of covid cases is very low. To address this issue, we will try class weigthing and oversampling later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Data preparation \n",
    "\n",
    "Repeating the steps from the previous notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>pix_value</th>\n",
       "      <th>resized_pix</th>\n",
       "      <th>blurred_pix</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[[100, 90, 79, 73, 70, 66, 66, 69, 73, 73, 73,...</td>\n",
       "      <td>[[98, 82, 72, 67, 66, 72, 73, 73, 74, 75, 76, ...</td>\n",
       "      <td>[[88, 83, 75, 70, 70, 71, 73, 74, 75, 77, 79, ...</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[[110, 130, 128, 124, 141, 142, 130, 132, 142,...</td>\n",
       "      <td>[[112, 129, 126, 137, 142, 130, 136, 137, 134,...</td>\n",
       "      <td>[[123, 125, 130, 134, 136, 135, 135, 136, 137,...</td>\n",
       "      <td>viral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[[119, 119, 118, 116, 114, 112, 109, 108, 104,...</td>\n",
       "      <td>[[119, 119, 119, 119, 119, 118, 117, 117, 116,...</td>\n",
       "      <td>[[119, 119, 119, 119, 119, 118, 118, 117, 116,...</td>\n",
       "      <td>viral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[[5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6,...</td>\n",
       "      <td>[[5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 4, 4,...</td>\n",
       "      <td>[[5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 5, 5, 4,...</td>\n",
       "      <td>bacterial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[[109, 108, 106, 107, 108, 109, 108, 106, 109,...</td>\n",
       "      <td>[[109, 108, 106, 107, 108, 109, 108, 106, 108,...</td>\n",
       "      <td>[[108, 108, 107, 107, 108, 108, 107, 107, 108,...</td>\n",
       "      <td>viral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                          pix_value  \\\n",
       "0   0  [[100, 90, 79, 73, 70, 66, 66, 69, 73, 73, 73,...   \n",
       "1   1  [[110, 130, 128, 124, 141, 142, 130, 132, 142,...   \n",
       "2   2  [[119, 119, 118, 116, 114, 112, 109, 108, 104,...   \n",
       "3   3  [[5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6,...   \n",
       "4   4  [[109, 108, 106, 107, 108, 109, 108, 106, 109,...   \n",
       "\n",
       "                                         resized_pix  \\\n",
       "0  [[98, 82, 72, 67, 66, 72, 73, 73, 74, 75, 76, ...   \n",
       "1  [[112, 129, 126, 137, 142, 130, 136, 137, 134,...   \n",
       "2  [[119, 119, 119, 119, 119, 118, 117, 117, 116,...   \n",
       "3  [[5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 4, 4,...   \n",
       "4  [[109, 108, 106, 107, 108, 109, 108, 106, 108,...   \n",
       "\n",
       "                                         blurred_pix      label  \n",
       "0  [[88, 83, 75, 70, 70, 71, 73, 74, 75, 77, 79, ...     normal  \n",
       "1  [[123, 125, 130, 134, 136, 135, 135, 136, 137,...      viral  \n",
       "2  [[119, 119, 119, 119, 119, 118, 118, 117, 116,...      viral  \n",
       "3  [[5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 5, 5, 4,...  bacterial  \n",
       "4  [[108, 108, 107, 107, 108, 108, 107, 107, 108,...      viral  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the images in the folder and connecting them to their label\n",
    "n_rows = train_labels.shape[0]\n",
    "img_list = [np.array(Image.open('./train/' + train_labels.loc[i, 'filename'])) for i in range(n_rows)]\n",
    "X = train_labels.drop(columns= ['filename'])\n",
    "X['pix_value'] = img_list\n",
    "X = X[['id', 'pix_value', 'label']]\n",
    "\n",
    "# Resizing the images\n",
    "H = []\n",
    "W = []\n",
    "for k in range(X.shape[0]):\n",
    "    shape = X.loc[k, 'pix_value'].shape\n",
    "    H.append(shape[0])\n",
    "    W.append(shape[1])\n",
    "new_W = int(np.median(W))\n",
    "new_H = int(H[W.index(np.median(W))])\n",
    "resized_pix_list = []\n",
    "for k in range(X.shape[0]):\n",
    "    dim = (new_H, new_W)\n",
    "    resized_pix_list.append(cv2.resize(X.loc[k, 'pix_value'], dim))\n",
    "X = X.assign(resized_pix = resized_pix_list)\n",
    "\n",
    "# Denoising the images using a blurring technique\n",
    "blurred_pix_list = []\n",
    "for k in range(X.shape[0]):\n",
    "    blurred_pix_list.append(cv2.GaussianBlur(X.loc[k, 'resized_pix'], (5, 5), 0))\n",
    "X = X.assign(blurred_pix = blurred_pix_list)\n",
    "\n",
    "X = X[['id', 'pix_value', 'resized_pix', 'blurred_pix', 'label']]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "\n",
    "Let's split our data into a training set and a validation set,and then standardize them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Splitting into training set and validation set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keeping the ids for later and separating the features from the target\n",
    "\n",
    "ids = X['id']\n",
    "X_resized = X[['resized_pix']]\n",
    "X_blurred = X[['blurred_pix']]\n",
    "y = LabelEncoder().fit_transform(X['label']) # encoding the target to classes 0, 1, 2, 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['bacterial', 'covid', 'normal', 'viral'], dtype=object),\n",
       " array([350,  77, 350, 350]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(X['label'].values, return_counts= True) # Order of the label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One split for the resized data\n",
    "\n",
    "X_train_r, X_val_r, y_train_r, y_val_r = train_test_split(X_resized, y, test_size= 0.2, \n",
    "                                                          random_state= 0, stratify= y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One split for the denoised data to compare later\n",
    "\n",
    "X_train_b, X_val_b, y_train_b, y_val_b = train_test_split(X_blurred, y, test_size= 0.2, \n",
    "                                                          random_state= 0, stratify= y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Standardization\n",
    "\n",
    "Considering the heterogenity of the images observed in the previous study, we are going to standardize our data to improve the uniformity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_list, std_list = [], []\n",
    "for i in range(X_train_r.shape[0]):\n",
    "    mean_list.append(np.mean(X_train_r.iloc[i, 0]))\n",
    "    std_list.append(np.std(X_train_r.iloc[i, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bertrandthia/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/Users/bertrandthia/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "mean, std = np.mean(mean_list), np.mean(std_list)\n",
    "X_train_r['std_pix'] = X_train_r.apply(lambda x: (x['resized_pix'] - mean) / std, axis= 1)\n",
    "X_val_r['std_pix'] = X_val_r.apply(lambda x: (x['resized_pix'] - mean) / std, axis= 1)\n",
    "\n",
    "X_train_r, X_val_r = X_train_r[['std_pix']], X_val_r[['std_pix']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Formatting the inputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(901, 706680)\n"
     ]
    }
   ],
   "source": [
    "resized_train, resized_val = [], []\n",
    "\n",
    "for i in range(X_train_r.shape[0]):\n",
    "    resized_train.append(X_train_r.iloc[i, 0].reshape(1, -1).flatten())\n",
    "for i in range(X_val_r.shape[0]): \n",
    "    resized_val.append(X_val_r.iloc[i, 0].reshape(1, -1).flatten())\n",
    "resized_train, resized_val = np.array(resized_train), np.array(resized_val)\n",
    "print(resized_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of X_train_r: (901, 755, 936, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train_r2 = resized_train.reshape(X_train_r.shape[0], *(new_H, new_W, 1))\n",
    "X_val_r2 = resized_val.reshape(X_val_r.shape[0], *(new_H, new_W, 1))\n",
    "X_train_r, X_val_r = X_train_r2, X_val_r2\n",
    "print('Size of X_train_r:', X_train_r.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bertrandthia/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), input_shape=(755, 936,..., activation=\"relu\")`\n",
      "  \n",
      "/Users/bertrandthia/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=32)`\n",
      "  \"\"\"\n",
      "/Users/bertrandthia/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"sigmoid\", units=4)`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "bas_model = Sequential()\n",
    "bas_model.add(Conv2D(32, 3, 3, input_shape= (new_H, new_W, 1), activation= 'relu')) \n",
    "bas_model.add(MaxPooling2D(pool_size= (2, 2)))\n",
    "bas_model.add(Flatten())\n",
    "bas_model.add(Dense(output_dim= 32, activation= 'relu')) \n",
    "bas_model.add(Dense(output_dim= 4, activation= 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 753, 934, 32)      320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 376, 467, 32)      0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 5618944)           0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 32)                179806240 \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4)                 132       \n",
      "=================================================================\n",
      "Total params: 179,806,692\n",
      "Trainable params: 179,806,692\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bas_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "bas_model.compile(loss= 'sparse_categorical_crossentropy',\n",
    "                 optimizer= Adam(), \n",
    "                 metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs= 5\n",
    "es = EarlyStopping(monitor='val_accuracy', mode='max', verbose=1, restore_best_weights= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 901 samples, validate on 226 samples\n",
      "Epoch 1/5\n",
      "384/901 [===========>..................] - ETA: 12:50 - loss: 6.3780 - accuracy: 0.2891"
     ]
    }
   ],
   "source": [
    "bas_model.fit(X_train_r, y_train_r, batch_size= 32, \n",
    "              epochs= epochs, verbose= 1, validation_data = (X_val_r, y_val_r), callbacks= [es])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
